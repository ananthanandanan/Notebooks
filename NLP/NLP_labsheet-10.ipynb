{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203edf0d",
   "metadata": {},
   "source": [
    "### K N Anantha nandanan\n",
    "#### Roll No AM.EN.U4CSE19326\n",
    "\n",
    "BBC news clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928d5c0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "714ad109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754aeb6a",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a62f4b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../Datas/bbc-text.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919871a5",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01ba3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(text):\n",
    "    #print(type(text))\n",
    "    stemm_snow = SnowballStemmer(\"english\")\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    #text = [stemm_snow.stem(word) for word in text]\n",
    "    text = [lem.lemmatize(word) for word in text]\n",
    "    text = [char for char in text if char not in string.punctuation]\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6adf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(preProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c691c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rushed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos left book alone former worldcom b...  \n",
       "2  tiger wary farrell gamble leicester say rushed...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0351c9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "798c0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs =[]\n",
    "for d in df.clean_text:\n",
    "    docs.append(d.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aad084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove both rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the doc\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb215417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6079c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2973\n",
      "Number of documents: 2225\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "664f161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel, LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90356f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None# Don't evaluate model perplexity, takes too much time.\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce98afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.019003633, 'mobile'), (0.017456427, 'phone')], -0.2033409239962164),\n",
      " ([(0.014300388, 'search'), (0.008270076, 'new')], -0.6286086593528429),\n",
      " ([(0.010778276, 'u'), (0.006428739, 'one')], -0.6943313133158895),\n",
      " ([(0.01165616, 'film'), (0.010448933, 'u')], -0.8690378470045925),\n",
      " ([(0.011175838, 'people'), (0.008026994, 'u')], -0.9737444140339473),\n",
      " ([(0.012527265, 'u'), (0.010251986, 'mr')], -1.0027643298708047),\n",
      " ([(0.009585452, 'u'), (0.009432481, 'month')], -1.2380784168033652),\n",
      " ([(0.010201984, 'player'), (0.008070503, 'england')], -1.4601993905789359),\n",
      " ([(0.027912728, 'mr'), (0.011052411, 'blair')], -1.6413895122262547),\n",
      " ([(0.013202245, 'mr'), (0.010955351, 'game')], -2.401675995607374)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus,topn=2)\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1498ccc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5458dc",
   "metadata": {},
   "source": [
    "### LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34f4413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSI model.\n",
    "model = LsiModel(\n",
    "    corpus=corpus,id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95a810c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.300*\"mr\" + 0.231*\"people\" + 0.171*\"new\" + 0.161*\"one\" + 0.147*\"u\"'),\n",
       " (1,\n",
       "  '-0.504*\"mr\" + 0.234*\"game\" + 0.219*\"music\" + 0.213*\"best\" + -0.208*\"labour\"'),\n",
       " (2,\n",
       "  '0.450*\"best\" + 0.350*\"song\" + 0.238*\"award\" + -0.206*\"game\" + 0.203*\"music\"'),\n",
       " (3,\n",
       "  '-0.518*\"game\" + 0.285*\"music\" + 0.216*\"people\" + 0.183*\"mobile\" + 0.169*\"phone\"'),\n",
       " (4,\n",
       "  '0.288*\"u\" + -0.246*\"mr\" + -0.223*\"game\" + -0.206*\"mobile\" + -0.196*\"music\"')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65ed4b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.12003839476702446),\n",
       " (1, 0.1430590665313521),\n",
       " (2, -0.06687045827921333),\n",
       " (3, -0.2916993855803869),\n",
       " (4, -0.009322990375509072)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [  \n",
    "    ['computer', 'time', 'mobile'],  \n",
    "    ['vote', 'election', 'candidate'],  \n",
    "    ['defeat', 'player', 'cup', 'england','cricket']]\n",
    "other_corpus = [dictionary.doc2bow(text) for text in other_texts]\n",
    "unseen_doc = other_corpus[2]\n",
    "vector = model[unseen_doc]  # get topic probability distribution for a document\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "907a5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.12003839476702446), (1, 0.1430590665313521), (2, -0.06687045827921333), (3, -0.2916993855803869), (4, -0.009322990375509072)]\n"
     ]
    }
   ],
   "source": [
    "for d in other_corpus:\n",
    "    vector = model[d]\n",
    "print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
