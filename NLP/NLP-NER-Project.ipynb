{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9ff894",
   "metadata": {},
   "source": [
    "## Named Entity Recongnition(NER) \n",
    "\n",
    "### What is Named Entity ?\n",
    "\n",
    "**Any word which represents a person, organization, location etc, is a Named Entity. Named entiy recognition is a subtask of information extraction and is the process of identifying words which are named entities in a given test. It is also called entity identification or entity chunking.**\n",
    "\n",
    "--- \n",
    "\n",
    "### Example\n",
    "\n",
    "**\"Apple acquired Zoom in China on Wednesday 6th May 2020\"**\n",
    "- Here named entites are Apple, Zoom, China and Wednesday 6th May 2020\n",
    "- Named entity recongnition is the task of identifying these words from the text.\n",
    "---\n",
    "\n",
    "### Why is it important ?\n",
    "\n",
    "**In order to understand the meaning from a given text, it is important to indentify who did what to whom. Named entity recognition is the first task of identifying the words which may represent the who, what and whom in the text. It helps in indentifying the major entities the text is talking about.**\n",
    "\n",
    "\n",
    "**What this means is that, any NLP task which involves automatically understanding text and acts baed on it, needs the NER or Named Entity Recognition in its pipeline.**\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "### Team ideation \n",
    "\n",
    "we have understood that there are different ways to build the model for NER, but we will go with mainly both NLTK and Spacy approach and compare which gives the better results, though both are not perfect.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Approaches\n",
    "1. NLTK\n",
    "    a. Word based segmentation\n",
    "    b. Sentence based segmentation - Sentence allowes entities to be formed out of context, rather than depending just on the meaning of only the word.\n",
    "2. Spacy\n",
    "3. Using Stanford NLP NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0938358",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94ae7a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ananthan2k/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ananthan2k/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ananthan2k/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "## Spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37976ceb",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "NOTE: For the ease of debugging the text we have used a small text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f6d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple acquired Zoom in China on Wednesday 6th May 2020.\\\n",
    "This news has made Apple and Google stock jump by 5% on Dow Jones Index in the \\\n",
    "United States of America\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159bf07f",
   "metadata": {},
   "source": [
    "**NER using word segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f159bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'acquired',\n",
       " 'Zoom',\n",
       " 'in',\n",
       " 'China',\n",
       " 'on',\n",
       " 'Wednesday',\n",
       " '6th',\n",
       " 'May',\n",
       " '2020.This',\n",
       " 'news',\n",
       " 'has',\n",
       " 'made',\n",
       " 'Apple',\n",
       " 'and',\n",
       " 'Google',\n",
       " 'stock',\n",
       " 'jump',\n",
       " 'by',\n",
       " '5',\n",
       " '%',\n",
       " 'on',\n",
       " 'Dow',\n",
       " 'Jones',\n",
       " 'Index',\n",
       " 'in',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'of',\n",
       " 'America']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d48608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'NNP'),\n",
       " ('acquired', 'VBD'),\n",
       " ('Zoom', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('China', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('6th', 'CD'),\n",
       " ('May', 'NNP'),\n",
       " ('2020.This', 'CD'),\n",
       " ('news', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('made', 'VBN'),\n",
       " ('Apple', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Google', 'NNP'),\n",
       " ('stock', 'NN'),\n",
       " ('jump', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('5', 'CD'),\n",
       " ('%', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Dow', 'NNP'),\n",
       " ('Jones', 'NNP'),\n",
       " ('Index', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('of', 'IN'),\n",
       " ('America', 'NNP')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323a929",
   "metadata": {},
   "source": [
    "**Next we can chunk the tags. So here is the part we classify the pos_tagged words into named entities.\n",
    "We will follow two ways:\n",
    "1. Binary=True -> So here either a pos tagged word is NE or not NE. And just indicates NE words labelled as 'NE'\n",
    "2. Binary=False -> All pos tagged as identified as Named Entity, in some form**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb230e3",
   "metadata": {},
   "source": [
    "**Binary=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af1237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Apple/NNP)\n",
      "('acquired', 'VBD')\n",
      "('Zoom', 'NNP')\n",
      "('in', 'IN')\n",
      "(NE China/NNP)\n",
      "('on', 'IN')\n",
      "('Wednesday', 'NNP')\n",
      "('6th', 'CD')\n",
      "('May', 'NNP')\n",
      "('2020.This', 'CD')\n",
      "('news', 'NN')\n",
      "('has', 'VBZ')\n",
      "('made', 'VBN')\n",
      "(NE Apple/NNP)\n",
      "('and', 'CC')\n",
      "(NE Google/NNP)\n",
      "('stock', 'NN')\n",
      "('jump', 'NN')\n",
      "('by', 'IN')\n",
      "('5', 'CD')\n",
      "('%', 'NN')\n",
      "('on', 'IN')\n",
      "('Dow', 'NNP')\n",
      "('Jones', 'NNP')\n",
      "('Index', 'NNP')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "(NE United/NNP States/NNPS)\n",
      "('of', 'IN')\n",
      "(NE America/NNP)\n"
     ]
    }
   ],
   "source": [
    "chunks = ne_chunk(pos_tags, binary=True)\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1cd4f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_frozen_class', '_get_node', '_label', '_parse_error', '_pformat_flat', '_repr_png_', '_set_node', 'append', 'chomsky_normal_form', 'clear', 'collapse_unary', 'convert', 'copy', 'count', 'draw', 'extend', 'flatten', 'freeze', 'fromlist', 'fromstring', 'height', 'index', 'insert', 'label', 'leaf_treeposition', 'leaves', 'node', 'pformat', 'pformat_latex_qtree', 'pop', 'pos', 'pprint', 'pretty_print', 'productions', 'remove', 'reverse', 'set_label', 'sort', 'subtrees', 'treeposition_spanning_leaves', 'treepositions', 'un_chomsky_normal_form']\n"
     ]
    }
   ],
   "source": [
    "print(dir(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc0fe805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>America</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entities Labels\n",
       "0          China     NE\n",
       "1          Apple     NE\n",
       "2        America     NE\n",
       "3         Google     NE\n",
       "4  United States     NE"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        #print(chunk)\n",
    "        #print(chunk[0])\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "#print(entities_labels)\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f187e",
   "metadata": {},
   "source": [
    "**Binary=False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccae362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Apple/NNP)\n",
      "('acquired', 'VBD')\n",
      "(PERSON Zoom/NNP)\n",
      "('in', 'IN')\n",
      "(GPE China/NNP)\n",
      "('on', 'IN')\n",
      "('Wednesday', 'NNP')\n",
      "('6th', 'CD')\n",
      "('May', 'NNP')\n",
      "('2020.This', 'CD')\n",
      "('news', 'NN')\n",
      "('has', 'VBZ')\n",
      "('made', 'VBN')\n",
      "(PERSON Apple/NNP)\n",
      "('and', 'CC')\n",
      "(ORGANIZATION Google/NNP)\n",
      "('stock', 'NN')\n",
      "('jump', 'NN')\n",
      "('by', 'IN')\n",
      "('5', 'CD')\n",
      "('%', 'NN')\n",
      "('on', 'IN')\n",
      "(PERSON Dow/NNP Jones/NNP Index/NNP)\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "(GPE United/NNP States/NNPS)\n",
      "('of', 'IN')\n",
      "(GPE America/NNP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>America</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dow Jones Index</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entities        Labels\n",
       "0           Google  ORGANIZATION\n",
       "1            China           GPE\n",
       "2    United States           GPE\n",
       "3          America           GPE\n",
       "4  Dow Jones Index        PERSON\n",
       "5             Zoom        PERSON\n",
       "6            Apple        PERSON"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = ne_chunk(pos_tags, binary=False) #either NE or not NE\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    \n",
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542a7de",
   "metadata": {},
   "source": [
    "**NE based on sentence segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cc472c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>America</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dow Jones Index</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entities        Labels\n",
       "0           Google  ORGANIZATION\n",
       "1            China           GPE\n",
       "2    United States           GPE\n",
       "3          America           GPE\n",
       "4  Dow Jones Index        PERSON\n",
       "5             Zoom        PERSON\n",
       "6            Apple        PERSON"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "labels = []\n",
    "\n",
    "sentence = sent_tokenize(text)\n",
    "for sent in sentence:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)),binary=False):\n",
    "        if hasattr(chunk,'label'):\n",
    "            entities.append(' '.join(c[0] for c in chunk))\n",
    "            labels.append(chunk.label())\n",
    "            \n",
    "entities_labels = list(set(zip(entities,labels)))\n",
    "\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f3e16",
   "metadata": {},
   "source": [
    "### Spacy based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd847cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download spacy models\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd2a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5292d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('Zoom', 'ORG'), ('China', 'GPE'), ('Wednesday 6th', 'DATE'), ('Apple', 'ORG'), ('5%', 'PERCENT'), ('Dow Jones Index', 'ORG'), ('the United States of America', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "## Feed the text\n",
    "doc = nlp(text)\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf7285be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ORG', 'ORG', 'GPE', 'DATE', 'ORG', 'PERCENT', 'ORG', 'GPE']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels = [x.label_ for x in doc.ents]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7fef15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Apple acquired Zoom in China on Wednesday 6th,\n",
       " May 2020.This news has made Apple and Google stock jump by 5% on Dow Jones Index in the United States of America]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [sent for sent in doc.sents]\n",
    "sentence\n",
    "#len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9569ce75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " acquired \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zoom\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    China\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wednesday 6th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " May 2020.This news has made \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and Google stock jump by \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    5%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dow Jones Index\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States of America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(text), jupyter=True, style='ent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
